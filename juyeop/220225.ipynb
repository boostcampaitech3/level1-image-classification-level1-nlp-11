{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f7f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82372af-e478-4dae-a283-f792958f76f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = 'input/data/train'\n",
    "train_image_dir_path = os.path.join(train_path, 'images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e84dc1-18eb-4592-bd15-330007a90b01",
   "metadata": {},
   "source": [
    "Dataset 생성\n",
    "\n",
    "모든 train data의 path를 가져와 라벨링 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "040577b9-a2c5-4516-a5fd-b1f3ae11d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(dirname, result): # 하위 목록의 모든 파일을 찾는 함수\n",
    "    try:\n",
    "        filenames = os.listdir(dirname)\n",
    "        for filename in filenames:\n",
    "            if filename[0] == '.': # .으로 시작하는 애들 거름\n",
    "                continue\n",
    "            full_filename = os.path.join(dirname, filename)\n",
    "            if os.path.isdir(full_filename):\n",
    "                search(full_filename, result)\n",
    "            else:\n",
    "                ext = os.path.splitext(full_filename)[-1] # 확장자 체크\n",
    "                if ext:\n",
    "                    result.append(full_filename)\n",
    "        \n",
    "    except PermissionError:\n",
    "        print('Permission Error')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e40f2b1-7abe-49e1-abda-701cb6025934",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_path = list()\n",
    "search(train_image_dir_path, all_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455a861-4429-4243-898c-b3f2e4186942",
   "metadata": {},
   "source": [
    "train의 데이터 디렉토리는 2700개로, 각각의 이미지 파일(incorrect, mask1~5, normal)을 곱한 갯수가 나옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba63e19e-6978-4785-921e-44a58757f838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18900"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_path) # 2700 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff701504-72d2-44d0-8e2e-82ac0964e1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input/data/train/images/001752_male_Asian_53/mask5.jpg',\n",
       " 'input/data/train/images/001752_male_Asian_53/mask4.jpg',\n",
       " 'input/data/train/images/001752_male_Asian_53/mask2.jpg',\n",
       " 'input/data/train/images/001752_male_Asian_53/mask3.jpg',\n",
       " 'input/data/train/images/001752_male_Asian_53/normal.jpg',\n",
       " 'input/data/train/images/001752_male_Asian_53/mask1.jpg',\n",
       " 'input/data/train/images/001752_male_Asian_53/incorrect_mask.jpg',\n",
       " 'input/data/train/images/005127_female_Asian_51/mask5.jpg',\n",
       " 'input/data/train/images/005127_female_Asian_51/mask4.jpg',\n",
       " 'input/data/train/images/005127_female_Asian_51/mask2.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_path[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8726c41-ba96-408b-9e65-10bcab850853",
   "metadata": {},
   "source": [
    "파일의 확장자는 jpg, png, jpeg로 3종류가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b45de68-c50e-435b-853b-5e95fba44852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.jpg', '.png', '.jpeg']\n"
     ]
    }
   ],
   "source": [
    "exts = list()\n",
    "for word in all_path:\n",
    "    ext = os.path.splitext(word)[-1]\n",
    "    if ext not in exts:\n",
    "        exts.append(ext)\n",
    "print(exts) # jpg, png, jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f205b726-45fc-4d95-b3d0-fef15a7ecf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_path = sorted(all_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e47982-6d3f-4677-abfe-e9997e699876",
   "metadata": {},
   "source": [
    "라벨링을 하는 함수입니다. 조건에 따라 label에 숫자를 더해주는 식으로 만들었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370658c3-e841-4e0d-809b-c53d00cd9337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(name):\n",
    "    label = 0\n",
    "    info, mask_type = name.split('/')[-2:]\n",
    "    info = info.split('_')\n",
    "    gender, age = info[1], int(info[3])\n",
    "    \n",
    "    # 마스크 구별\n",
    "    if 'incorrect' in mask_type:\n",
    "        label += 6\n",
    "    elif 'normal' in mask_type:\n",
    "        label += 12\n",
    "    \n",
    "    # gender 구별\n",
    "    if gender == 'female':\n",
    "        label += 3\n",
    "    \n",
    "    # 나이 구별\n",
    "    if 30 <= age and age < 60:\n",
    "        label += 1\n",
    "    elif age >= 60:\n",
    "        label += 1\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c06615-2768-4cfd-9026-6105537d0dcb",
   "metadata": {},
   "source": [
    "path, label을 컬럼으로 갖는 dataframe을 생성해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a01df2-a77e-4390-8a42-9465893317fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>input/data/train/images/000001_female_Asian_45...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>input/data/train/images/000001_female_Asian_45...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>input/data/train/images/000001_female_Asian_45...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>input/data/train/images/000001_female_Asian_45...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>input/data/train/images/000001_female_Asian_45...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>input/data/train/images/006959_male_Asian_19/m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>input/data/train/images/006959_male_Asian_19/m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>input/data/train/images/006959_male_Asian_19/m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>input/data/train/images/006959_male_Asian_19/m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>input/data/train/images/006959_male_Asian_19/n...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  label\n",
       "0      input/data/train/images/000001_female_Asian_45...     10\n",
       "1      input/data/train/images/000001_female_Asian_45...      4\n",
       "2      input/data/train/images/000001_female_Asian_45...      4\n",
       "3      input/data/train/images/000001_female_Asian_45...      4\n",
       "4      input/data/train/images/000001_female_Asian_45...      4\n",
       "...                                                  ...    ...\n",
       "18895  input/data/train/images/006959_male_Asian_19/m...      0\n",
       "18896  input/data/train/images/006959_male_Asian_19/m...      0\n",
       "18897  input/data/train/images/006959_male_Asian_19/m...      0\n",
       "18898  input/data/train/images/006959_male_Asian_19/m...      0\n",
       "18899  input/data/train/images/006959_male_Asian_19/n...     12\n",
       "\n",
       "[18900 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path_label = pd.DataFrame(all_path, columns = ['path'])\n",
    "\n",
    "train_path_label['label'] = train_path_label['path'].map(lambda x: labeling(x))\n",
    "train_path_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897da325-d4df-495c-937c-d195424edb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path_label.to_csv('./train_path_label.csv', index=False, encoding='utf-8')\n",
    "# train_path_label = pd.read_csv('./train_path_label.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45d570-0645-4bd1-9623-797a0a287648",
   "metadata": {},
   "source": [
    "dataset을 상속받아 만든 CustomDataset입니다. transform은 size를 [512, 384]로 변형하고, Tensor로 만들고, 정규화를 해주었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfa99111-9bb5-4f27-b062-956f1b61d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_paths_label, transform):\n",
    "        self.X = img_paths_label['path']\n",
    "        self.y = img_paths_label['label']\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.X.iloc[index])\n",
    "        label = self.y.iloc[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74229d34-afe4-49b3-a84c-93885000beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1da396-de7c-46e3-b4a7-9d3a47bee7d2",
   "metadata": {},
   "source": [
    "train, valid를 나누는 부분입니다.\n",
    "\n",
    "label의 비율을 유지하면서 나눴습니다.\n",
    "\n",
    "+ 기존 방법 대신StratifiedKFold 사용을 시도했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f983476-59fd-485c-b5bd-a3755af6cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train, valid = train_test_split(train_path_label, test_size=0.2,\n",
    "#                                shuffle=True, stratify=train_path_label['label'],\n",
    "#                                random_state=34)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53192a6-95ba-431b-80c7-85b820aab853",
   "metadata": {},
   "source": [
    "DataLoader\n",
    "- index를 사용한 Dataloader 정의\n",
    "- getDataloader 함수 설명\n",
    "    1. Pytorch Dataset, train 인덱스, valid 인덱스, batch size를 전달받아 Train, Valid DataLoader 객체를 반환합니다.\n",
    "    2. torch.utils.data.Subset 객체는 데이터셋과 해당 데이터셋의 인덱스를 전달받아 Subset 객체를 생성합니다. 생성한 Subset 객체를 사용해 DataLoader 객체를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d5e3ee2-3afe-469d-b248-27b77e87d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataloader(dataset, train_idx, valid_idx, batch_size, num_workers):\n",
    "    # 인자로 전달받은 dataset에서 train_idx에 해당하는 Subset 추출\n",
    "    train_set = torch.utils.data.Subset(dataset,\n",
    "                                        indices=train_idx)\n",
    "    # 인자로 전달받은 dataset에서 valid_idx에 해당하는 Subset 추출\n",
    "    val_set   = torch.utils.data.Subset(dataset,\n",
    "                                        indices=valid_idx)\n",
    "    \n",
    "    # 추출된 Train Subset으로 DataLoader 생성\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "    # 추출된 Valid Subset으로 DataLoader 생성\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 생성한 DataLoader 반환\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44331d5-b6ae-403d-b3b1-c76a7758d53e",
   "metadata": {},
   "source": [
    "dataloader를 정의했습니다. batchsize는 64로 했고 shuffle을 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec86e71c-2975-47b0-bd06-28df82c791e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01bfaea4-9ff5-42c4-af4f-1f584af88e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(train_path_label, transform)\n",
    "\n",
    "custom_dataloader = DataLoader(dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e466688b-3615-422b-ad07-821a8d6c491d",
   "metadata": {},
   "source": [
    "모델\n",
    "모델은 pretrain된 resnet18을 가져왔습니다. 이 모델의 마지막 fc층만 저희의 과제인 18개의 class로 변경해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e76528b7-b699-491c-8507-a1d6f4a0d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "998b8581-1351-4a2c-aba4-08a74c4a17b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "OUTPUT_CLASS_NUM = 18\n",
    "resnet18.fc = torch.nn.Linear(in_features=512, out_features=OUTPUT_CLASS_NUM, bias=True) # output 18개로\n",
    "\n",
    "# xavier uniform\n",
    "torch.nn.init.xavier_uniform_(resnet18.fc.weight)\n",
    "stdv = 1. / math.sqrt(resnet18.fc.weight.size(1))\n",
    "resnet18.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "resnet18.fc.weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "58a6c46b-00fd-434d-97f8-17f87cbc9ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89442b2-35cf-4384-9d18-18481d60f01a",
   "metadata": {},
   "source": [
    "아래 대부분의 코드가 부스트캠프에서 학습 자료나 과제로 제공받았던 코드를 거의 그대로 사용했습니다.\n",
    "\n",
    "설명도 주석도 잘 달려 있어서 그대로 가져왔습니다.\n",
    "\n",
    "epoch는 5, lr은 0.0001로 주었습니다.\n",
    "\n",
    "추후에 lr scheduler로 lr을 변경해보는 방법도 좋을 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fe33dbb7-a718-4d57-b51c-79961d4ab129",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001 # 학습 때 사용하는 optimizer의 학습률 옵션 설정\n",
    "NUM_EPOCH = 3 # 학습 때 mnist train data set을 얼마나 많이 학습할 지 결정하는 옵션\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # 분류 학습 때 많이 사용되는 Cross Entropy Loss를 objective function으로 사용\n",
    "optimizer = torch.optim.Adam(resnet18.parameters(), lr=LEARNING_RATE) # weight 업데이트를 위한 optimizer를 Adam으로 사용함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "57d55cbe-9d85-4c5d-9ac0-12b8f9b09ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    path  label\n",
      "0      input/data/train/images/000001_female_Asian_45...     10\n",
      "1      input/data/train/images/000001_female_Asian_45...      4\n",
      "2      input/data/train/images/000001_female_Asian_45...      4\n",
      "3      input/data/train/images/000001_female_Asian_45...      4\n",
      "4      input/data/train/images/000001_female_Asian_45...      4\n",
      "...                                                  ...    ...\n",
      "18895  input/data/train/images/006959_male_Asian_19/m...      0\n",
      "18896  input/data/train/images/006959_male_Asian_19/m...      0\n",
      "18897  input/data/train/images/006959_male_Asian_19/m...      0\n",
      "18898  input/data/train/images/006959_male_Asian_19/m...      0\n",
      "18899  input/data/train/images/006959_male_Asian_19/n...     12\n",
      "\n",
      "[18900 rows x 2 columns]\n",
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss: 0.150, 평균 Accuracy: 0.956\n",
      "소요 시간: 2분 26초\n",
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss: 0.629, 평균 Accuracy: 0.765\n",
      "소요 시간: 2분 51초\n",
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss: 0.090, 평균 Accuracy: 0.978\n",
      "소요 시간: 5분 17초\n",
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss: 0.494, 평균 Accuracy: 0.810\n",
      "소요 시간: 5분 42초\n",
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss: 0.042, 평균 Accuracy: 0.992\n",
      "소요 시간: 8분 8초\n",
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss: 0.207, 평균 Accuracy: 0.945\n",
      "소요 시간: 8분 34초\n",
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss: 0.237, 평균 Accuracy: 0.924\n",
      "소요 시간: 10분 58초\n",
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss: 0.308, 평균 Accuracy: 0.912\n",
      "소요 시간: 11분 24초\n",
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss: 0.088, 평균 Accuracy: 0.975\n",
      "소요 시간: 13분 47초\n",
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss: 0.266, 평균 Accuracy: 0.921\n",
      "소요 시간: 14분 12초\n",
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss: 0.024, 평균 Accuracy: 0.996\n",
      "소요 시간: 16분 38초\n",
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss: 0.175, 평균 Accuracy: 0.942\n",
      "소요 시간: 17분 3초\n",
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss: 0.043, 평균 Accuracy: 0.988\n",
      "소요 시간: 19분 29초\n",
      "현재 epoch-3의 test-데이터 셋에서 평균 Loss: 0.113, 평균 Accuracy: 0.957\n",
      "소요 시간: 19분 55초\n",
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss: 0.011, 평균 Accuracy: 0.998\n",
      "소요 시간: 22분 18초\n",
      "현재 epoch-3의 test-데이터 셋에서 평균 Loss: 0.124, 평균 Accuracy: 0.962\n",
      "소요 시간: 22분 44초\n",
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss: 0.006, 평균 Accuracy: 1.000\n",
      "소요 시간: 25분 8초\n",
      "현재 epoch-3의 test-데이터 셋에서 평균 Loss: 0.059, 평균 Accuracy: 0.978\n",
      "소요 시간: 25분 34초\n",
      "현재 epoch-4의 train-데이터 셋에서 평균 Loss: 0.015, 평균 Accuracy: 0.997\n",
      "소요 시간: 27분 57초\n",
      "현재 epoch-4의 test-데이터 셋에서 평균 Loss: 0.024, 평균 Accuracy: 0.994\n",
      "소요 시간: 28분 25초\n",
      "현재 epoch-4의 train-데이터 셋에서 평균 Loss: 0.006, 평균 Accuracy: 1.000\n",
      "소요 시간: 30분 47초\n",
      "현재 epoch-4의 test-데이터 셋에서 평균 Loss: 0.024, 평균 Accuracy: 0.996\n",
      "소요 시간: 31분 14초\n",
      "현재 epoch-4의 train-데이터 셋에서 평균 Loss: 0.003, 평균 Accuracy: 1.000\n",
      "소요 시간: 33분 35초\n",
      "현재 epoch-4의 test-데이터 셋에서 평균 Loss: 0.008, 평균 Accuracy: 0.999\n",
      "소요 시간: 34분 3초\n",
      "현재 epoch-5의 train-데이터 셋에서 평균 Loss: 0.002, 평균 Accuracy: 1.000\n",
      "소요 시간: 36분 29초\n",
      "현재 epoch-5의 test-데이터 셋에서 평균 Loss: 0.062, 평균 Accuracy: 0.982\n",
      "소요 시간: 36분 54초\n",
      "현재 epoch-5의 train-데이터 셋에서 평균 Loss: 0.001, 평균 Accuracy: 1.000\n",
      "소요 시간: 39분 20초\n",
      "현재 epoch-5의 test-데이터 셋에서 평균 Loss: 0.056, 평균 Accuracy: 0.983\n",
      "소요 시간: 39분 44초\n",
      "현재 epoch-5의 train-데이터 셋에서 평균 Loss: 0.001, 평균 Accuracy: 1.000\n",
      "소요 시간: 42분 8초\n",
      "현재 epoch-5의 test-데이터 셋에서 평균 Loss: 0.068, 평균 Accuracy: 0.977\n",
      "소요 시간: 42분 33초\n",
      "학습 종료!\n",
      "최고 accuracy: 0.9986772537231445, 최고 낮은 loss: 0.008068210174447843\n",
      "소요 시간: 42분 33초\n"
     ]
    }
   ],
   "source": [
    "# 5-fold Stratified KFold 5개의 fold를 형성하고 5번 Cross Validation을 진행합니다.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "best_test_accuracy = 0.\n",
    "best_test_loss = 9999.\n",
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "# skf 설정\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "# kfold = KFold(n_splits=4, shuffle=False)\n",
    "\n",
    "# skf에서 사용할 labels 설정\n",
    "labels = [i for i in train_path_label['label']]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "num_workers = 4\n",
    "\n",
    "print(train_path_label)\n",
    "temp_idx = 0\n",
    "for train_index, validate_index in skf.split(train_path_label, labels):\n",
    "#     print(train_index, validate_index)\n",
    "    temp_idx += 1\n",
    "    train = train_path_label.iloc[train_index]\n",
    "    valid = train_path_label.iloc[validate_index]\n",
    "\n",
    "    train_dataset = CustomDataset(train, transform)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=False\n",
    "                                 )\n",
    "\n",
    "    valid_dataset = CustomDataset(valid, transform)\n",
    "\n",
    "    valid_dataloader = DataLoader(valid_dataset,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  shuffle=False)\n",
    "\n",
    "    dataloaders = {\n",
    "        \"train\": train_dataloader,\n",
    "        \"test\": valid_dataloader,\n",
    "    }\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "\n",
    "        for phase in [\"train\", \"test\"]:\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "#             # 네트워크 모델을 train 모드로 두어 gradient를 계산하고, \n",
    "#             # 여러 sub module (배치 정규화, 드롭아웃 등)이 train_mode로 작동할 수 있게 함.\n",
    "            if phase == \"train\":\n",
    "                resnet18.train()\n",
    "            # 네트워크 모델을 eval 모드로 두어 여러 sub module들이 eval mode로 작동할 수 있게 함.\n",
    "            elif phase == \"test\":\n",
    "                resnet18.eval()\n",
    "            \n",
    "            for ind, (images, labels) in enumerate(dataloaders[phase]):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad() # parameter gradient를 업데이트 전 초기화함.\n",
    "\n",
    "                # train 모드일 시에는 gradient를 계산하고, 아닐 때는 gradient를 계산하지 않아 연산량 최소화\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    logits = resnet18(images)\n",
    "                    # 모델에서 linear 값으로 나오는 예측 값([0.9, 1.2, 3.2, 0.1, -0.1, ...])에서 최대 output index를 찾아 예측 레이블([2])로 변경함\n",
    "                    _, preds = torch.max(logits, 1)\n",
    "                    loss = loss_fn(logits, labels)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward() # 모델의 예측 값과 실제 값의 CrossEntropy 차이를 통해 gradient를 계산\n",
    "                        optimizer.step() # 계산된 gradient를 가지고 모델 업데이트\n",
    "\n",
    "                running_loss += loss.item() * images.size(0) # 한 Batch에서의 loss 값 저장\n",
    "                running_acc += torch.sum(preds == labels.data) # 한 Batch에서의 Accuracy 값 저장\n",
    "\n",
    "            # 한 epoch이 모두 종료되었을 때,\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "\n",
    "            seconds = int(time.time() - start)\n",
    "            print(f\"현재 epoch-{temp_idx}-{epoch}의 {phase}-데이터 셋에서 평균 Loss: {epoch_loss:.3f}, 평균 Accuracy: {epoch_acc:.3f}\")\n",
    "            print(f\"소요 시간: {seconds // 60}분 {seconds % 60}초\")  # 현재시각 - 시작시간 = 실행 시간\n",
    "#             print(f\"현재 epoch-{epoch}의 {phase}-데이터 셋에서 평균 Loss: {epoch_loss:.3f}, 평균 Accuracy: {epoch_acc:.3f}\")\n",
    "#             print(f\"소요 시간: {seconds // 60}분 {seconds % 60}초\")  # 현재시각 - 시작시간 = 실행 시간\n",
    "\n",
    "            # phase가 test일 때\n",
    "            if phase == \"test\":\n",
    "                # best accuracy 계산\n",
    "                if best_test_accuracy < epoch_acc:\n",
    "                    best_test_accuracy = epoch_acc\n",
    "                # best loss 계산\n",
    "                if best_test_loss > epoch_loss:\n",
    "                    best_test_loss = epoch_loss\n",
    "\n",
    "                \n",
    "seconds = int(time.time() - start)\n",
    "print(\"학습 종료!\")\n",
    "print(f\"최고 accuracy: {best_test_accuracy}, 최고 낮은 loss: {best_test_loss}\")\n",
    "print(f\"소요 시간: {seconds // 60}분 {seconds % 60}초\")  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b8b5f-b9cc-489d-a8dc-6177bd582940",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394de42a-a2b6-467f-a8fe-5657afcedd28",
   "metadata": {},
   "source": [
    "dataloader를 정의했습니다. batchsize는 64로 했고 shuffle을 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "23ac036e-abaa-4b02-93ec-98f7f93106c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/input'\n",
    "\n",
    "torch.save(resnet18, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4b5867fb-4ace-4c4f-a078-be1d6b5ff4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=18, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3beba0d-98de-43af-9e9b-59f32bc88304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a2579-3491-418e-8fe7-1d744422374f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0299e3-3b3e-45b9-935f-33ff69f97011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c3ff1-45cc-4e72-b9b9-31e1e5ac207e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f018c8-9e3e-4ace-8ef0-f22293b82c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d2802f-43d8-4516-b4ce-ae2d69a75edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f95332-f0e4-42e1-ab83-1cebbc46f381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf64055-0028-4ab5-b34f-9bf6e7e399f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d31dd-d56c-4785-9fcb-09f6a37fe991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8298cf4c-f313-45cd-8549-7ecae3f11cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f74389-3912-4679-85c2-ce268e266dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c7477-c27f-4857-9b8b-c87c1f02647a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
